{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "244cf584-78db-4b52-83fb-74c0dc99c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Transformer Implementation\n",
    "from torch import nn\n",
    "import torch \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcf2b0ca-6084-4cda-8450-a74a1811203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Patch Embeddings \n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    split image into grids, and convert to a latent space vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, patch_size, in_channels, d_model, device):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size \n",
    "        self.patch_size = patch_size \n",
    "        self.in_channels = in_channels\n",
    "        self.d_model = d_model\n",
    "        # image_size should be dividable by patch_size \n",
    "        # default: image_size=400, patch_size=40\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2 \n",
    "\n",
    "        # using convolution to create non-overlapping patches\n",
    "        self.emb = nn.Conv2d(self.in_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size, device=device)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # input shape: (batch_size, in_channels, image_size, image_size) \n",
    "        # output shape: (batch_size, num_patches, out_channels)\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        # intermediate x's shape: (batch_size, seq_len, sqrt(num_patches), sqrt(num_patches))\n",
    "        \n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x \n",
    "\n",
    "## Positional Embeddings \n",
    "class PositionalEmbedding(nn.Module): \n",
    "    def __init__(self, d_model, image_size, patch_size, device):\n",
    "        super().__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.max_len = num_patches + 1\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, self.max_len, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, seq_len, _ = x.shape \n",
    "        return self.encoding[:seq_len, :]\n",
    "    \n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, d_model, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.patch_emb = PatchEmbedding(image_size, patch_size, in_channels, d_model, device)\n",
    "        self.pos_emb = PositionalEmbedding(d_model, image_size, patch_size, device)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        # Similar to BERT model, we should add CLS token to the starting of the sequence \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "    def forward(self, x): \n",
    "        batch_size, _, _, _ = x.shape\n",
    "        \n",
    "        x = self.patch_emb(x)\n",
    "        \n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # cls_token shape: (batch_size, 1, d_model)\n",
    "\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "\n",
    "        return self.dropout(x + pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e7dfb62-3160-4f85-b9d4-508ea3f06d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention Block \n",
    "class SelfAttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    A attention block with scale dot product attention for \n",
    "    Query, Key, Value \n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, eps=1e-12): \n",
    "        # Since this is not a NLP, we don't need any padding-mask or look-ahead mask \n",
    "        # also, q, k, v have the same shape \n",
    "        batch_size, n_head, seq_len, d_tensor = k.shape\n",
    "\n",
    "        k_T = k.transpose(2, 3)\n",
    "        att_weight = (q @ k_T) / math.sqrt(d_tensor)\n",
    "\n",
    "        # att_weight shape: batch_size, n_head, seq_len, d_tensor \n",
    "        att_score = self.softmax(att_weight)\n",
    "\n",
    "        return att_score @ v\n",
    "\n",
    "## Multihead Attention Block \n",
    "class MultiheadAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_head, d_model): \n",
    "        super().__init__()\n",
    "        self.n_head = n_head \n",
    "        self.d_model = d_model \n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention = SelfAttentionBlock()\n",
    "\n",
    "        self.Wconcat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split(self, tensor): \n",
    "        \"\"\"\n",
    "        split tensor into n_heads \n",
    "        (batch_size, seq_len, d_model) -> (batch_size, n_head, seq_len, d_tensor)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = tensor.shape \n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "\n",
    "        tensor = tensor.reshape(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "        \n",
    "        return tensor \n",
    "\n",
    "    def concat(self, tensor): \n",
    "        \"\"\"\n",
    "        reverse of split\n",
    "        (batch_size, n_head, seq_len, d_tensor) -> (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, n_head, seq_len, d_tensor = tensor.shape \n",
    "\n",
    "        tensor = tensor.transpose(1, 2).reshape(batch_size, seq_len, n_head * d_tensor)\n",
    "        \n",
    "        return tensor \n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        # apply Wq, Wk, Wv to get q, k, v \n",
    "        query = self.split(self.Wq(x))\n",
    "        key = self.split(self.Wk(x))\n",
    "        value = self.split(self.Wv(x))\n",
    "\n",
    "        # apply attention \n",
    "        out = self.attention(query, key, value)\n",
    "\n",
    "        out = self.concat(out)\n",
    "\n",
    "        out = self.Wconcat(out)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9991b777-8bfe-4c0c-8d52-fbcc5bc7e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define FeedForward Network \n",
    "class FeedForwardBlock(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, drop_prob=0.1): \n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, ffn_hidden)\n",
    "        self.linear2 = nn.Linear(ffn_hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4293e42-fc6f-4623-b7aa-38ff43743db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer EncoderBlock and Encoder \n",
    "class EncoderBlock(nn.Module): \n",
    "    def __init__(self, n_head, d_model, ffn_hidden, drop_prob=0.1): \n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.multihead_attn = MultiheadAttentionBlock(n_head, d_model)\n",
    "        self.ffn = FeedForwardBlock(d_model, ffn_hidden, drop_prob)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x \n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = self.multihead_attn(x)\n",
    "\n",
    "        x = self.dropout1(x + residual)\n",
    "        \n",
    "        residual = x \n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        x = self.dropout2(x + residual)\n",
    "\n",
    "        return x\n",
    "        \n",
    "class Encoder(nn.Module): \n",
    "    def __init__(self, image_size, patch_size, in_channels, n_head, d_model, ffn_hidden, n_layers, device,  drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(image_size, patch_size, in_channels, d_model, drop_prob, device)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(n_head, d_model, ffn_hidden, drop_prob) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.emb(x)\n",
    "\n",
    "        for layer in self.layers: \n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a0c2f55-38bc-4e65-a3ec-9ce992f86b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define MLP head for final classification \n",
    "class MLPHead(nn.Module): \n",
    "    def __init__(self, d_model, class_num, mlp_hidden, drop_prob): \n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, mlp_hidden)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(mlp_hidden, class_num)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x) \n",
    "        x = self.gelu(x) \n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "152e3a41-59e1-4bb6-a3d3-07bafc35be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module): \n",
    "    \"\"\"\n",
    "    ViT Model \n",
    "    \"\"\"\n",
    "    def __init__(self, image_size, patch_size, in_channels, n_head, d_model, ffn_hidden, mlp_hidden, n_layers, class_num, device, drop_prob=0.1): \n",
    "        super().__init__()\n",
    "        seq_len = (image_size // patch_size) ** 2 + 1\n",
    "        self.encoder = Encoder(image_size, patch_size, in_channels, n_head, d_model, ffn_hidden, n_layers, device, drop_prob)\n",
    "        self.mlp_head = MLPHead(d_model, class_num, mlp_hidden, drop_prob)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.encoder(img)\n",
    "\n",
    "        logits = self.mlp_head(x[:, 0, :])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949daf37-384b-4fa2-bdf2-ba55092eb8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
