{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1e2479-82b7-4ad9-af68-3cc68ed3bd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%run config.ipynb\n",
    "%run ViT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2593fbb0-ba1b-4be5-97dd-b0ccf4cae761",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_dir / \"cat.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebd27bd-1846-4c50-9161-03926f803016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee555305-acb0-48a3-b847-5fb3b473847b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (encoder): Encoder(\n",
       "    (emb): TransformerEmbedding(\n",
       "      (patch_emb): PatchEmbedding(\n",
       "        (emb): Conv2d(3, 400, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (pos_emb): PositionalEmbedding()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x EncoderBlock(\n",
       "        (norm): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (multihead_attn): MultiheadAttentionBlock(\n",
       "          (Wq): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (Wk): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (Wv): Linear(in_features=400, out_features=400, bias=True)\n",
       "          (attention): SelfAttentionBlock(\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (Wconcat): Linear(in_features=400, out_features=400, bias=True)\n",
       "        )\n",
       "        (ffn): FeedForwardBlock(\n",
       "          (linear1): Linear(in_features=400, out_features=512, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=400, bias=True)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (mlp_head): MLPHead(\n",
       "    (linear1): Linear(in_features=400, out_features=512, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (linear2): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ViT(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=IN_CHANNELS,\n",
    "    n_head=N_HEAD, \n",
    "    d_model=D_MODEL, \n",
    "    ffn_hidden=FFN_HIDDEN, \n",
    "    mlp_hidden=MLP_HIDDEN, \n",
    "    n_layers=N_LAYERS, \n",
    "    class_num=CLASS_NUM, \n",
    "    device=device, \n",
    "    drop_prob=DROP_PROB,\n",
    ")\n",
    "\n",
    "# load model\n",
    "model_path = model_dir / \"model_68percent_acc\"\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True, map_location=device))\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7c61a4-02bc-4875-858b-5402ad1b1b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of img: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# prepare single image tensor for input \n",
    "img_inference_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=0.5, std=0.5)\n",
    "])\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "img = img_inference_transform(img)\n",
    "\n",
    "# set batch_size as 1 using unsqueeze\n",
    "img = img.unsqueeze(0)\n",
    "print(f'shape of img: {img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3d9beb-9789-4320-86bb-b4b55f67728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n"
     ]
    }
   ],
   "source": [
    "# read the cifar10 classes.txt\n",
    "class_list = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "class_dict = dict()\n",
    "for idx, classname in enumerate(class_list):\n",
    "    class_dict[idx] = classname\n",
    "\n",
    "print(f'{class_dict}')\n",
    "\n",
    "# define a helper function \n",
    "def tensor2cifarid(tensor): \n",
    "    # convert tensor to foodid \n",
    "    # by picking index that has max value \n",
    "    # use argmax \n",
    "    cifar_id = torch.argmax(tensor, dim=-1)\n",
    "    return cifar_id\n",
    "\n",
    "def cifarid2cifarname(cifarid):\n",
    "    if cifarid < 0 or cifarid > max(class_dict.keys()):\n",
    "        raise Exception(\"Invalid food Id for foodid2foodname function\")\n",
    "    return class_dict[cifarid]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf0c6bc-0cbb-4f6d-8eb6-22b5f37bd012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, img):\n",
    "    with torch.no_grad():\n",
    "        img = img.to(device)\n",
    "\n",
    "        out = model(img)\n",
    "        cifarid = tensor2cifarid(out)\n",
    "        cifarname = cifarid2cifarname(cifarid.item())\n",
    "\n",
    "        print(f'{cifarname}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90b76465-baa2-4608-b3d9-68e0d5334e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "inference(model, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9592c2d-57a5-4a63-a6bb-294a13be5795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
