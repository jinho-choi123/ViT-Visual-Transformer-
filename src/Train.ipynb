{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5710d848-ca35-4c38-b76d-3d984b7a2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Vit using Food101 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f86868b-1470-42be-ae97-9cd3d7aa4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%run config.ipynb\n",
    "%run ViT.ipynb\n",
    "\n",
    "if DATASET==\"mnist\":\n",
    "    %run data-MNIST.ipynb\n",
    "    print(f'Using MNIST as dataset')\n",
    "elif DATASET==\"cifar10\": \n",
    "    %run data-CIFAR10.ipynb\n",
    "    print(f'Using CIFAR10 as dataset')\n",
    "else:\n",
    "    raise Exception(\"Invalid Configuration for DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97823546-22b7-45d8-845c-1a22a4c7a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam \n",
    "from datetime import datetime \n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20415737-0e48-457c-9988-b6d42d9727ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "ViT                                                [64, 10]                  --\n",
      "├─Encoder: 1-1                                     [64, 50, 200]             --\n",
      "│    └─TransformerEmbedding: 2-1                   [64, 50, 200]             200\n",
      "│    │    └─PatchEmbedding: 3-1                    [64, 49, 200]             3,400\n",
      "│    │    └─PositionalEmbedding: 3-2               [1, 50, 200]              10,000\n",
      "│    │    └─Dropout: 3-3                           [64, 50, 200]             --\n",
      "│    └─ModuleList: 2-2                             --                        --\n",
      "│    │    └─EncoderBlock: 3-4                      [64, 50, 200]             366,712\n",
      "│    │    └─EncoderBlock: 3-5                      [64, 50, 200]             366,712\n",
      "│    │    └─EncoderBlock: 3-6                      [64, 50, 200]             366,712\n",
      "│    │    └─EncoderBlock: 3-7                      [64, 50, 200]             366,712\n",
      "│    │    └─EncoderBlock: 3-8                      [64, 50, 200]             366,712\n",
      "│    └─LayerNorm: 2-3                              [64, 50, 200]             400\n",
      "├─MLPHead: 1-2                                     [64, 10]                  --\n",
      "│    └─Linear: 2-4                                 [64, 512]                 102,912\n",
      "│    └─GELU: 2-5                                   [64, 512]                 --\n",
      "│    └─Linear: 2-6                                 [64, 10]                  5,130\n",
      "│    └─Dropout: 2-7                                [64, 10]                  --\n",
      "====================================================================================================\n",
      "Total params: 1,955,602\n",
      "Trainable params: 1,955,602\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 135.08\n",
      "====================================================================================================\n",
      "Input size (MB): 0.20\n",
      "Forward/backward pass size (MB): 255.22\n",
      "Params size (MB): 7.82\n",
      "Estimated Total Size (MB): 263.24\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = ViT(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=IN_CHANNELS,\n",
    "    n_head=N_HEAD, \n",
    "    d_model=D_MODEL, \n",
    "    ffn_hidden=FFN_HIDDEN, \n",
    "    mlp_hidden=MLP_HIDDEN, \n",
    "    n_layers=N_LAYERS, \n",
    "    class_num=CLASS_NUM, \n",
    "    device=device, \n",
    "    drop_prob=DROP_PROB,\n",
    ")\n",
    "\n",
    "# load model \n",
    "if LOAD_MODEL:\n",
    "    loading_model_path = model_dir / LOADING_MODEL_NAME\n",
    "    model.load_state_dict(torch.load(loading_model_path, weights_only=True))\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "def count_parameters(model): \n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "\n",
    "logger.info(summary(model, input_size=(BATCH_SIZE, IN_CHANNELS, IMG_SIZE, IMG_SIZE)))\n",
    "print(summary(model, input_size=(BATCH_SIZE, IN_CHANNELS, IMG_SIZE, IMG_SIZE)))\n",
    "\n",
    "logger.info(f'model parameter #: {count_parameters(model)}')\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"Parameter {name} is on {param.device}\")\n",
    "# for name, buffer in model.named_buffers():\n",
    "#     print(f\"Buffer {name} is on {buffer.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b490b3-e88f-4599-b33b-7c1e530a517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer \n",
    "optimizer = Adam(params = model.parameters(), lr=INIT_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.99 ** epoch)\n",
    "# Setup loss function for training \n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac9adba-1e11-4070-a4b8-6192b8bfdcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_per_epoch = []\n",
    "test_loss_per_epoch = []\n",
    "test_accuracy_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e551b7f-8f6c-49dc-a3f2-8b6e1f25d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph of train_epoch_loss, test_epoch_loss, test_accuracy\n",
    "def plot_losses(loss_values, label):\n",
    "    x0 = list(range(1, len(loss_values)+1))\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot(x0, loss_values)\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab542c29-a836-47e1-812d-ea27008b282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch_num): \n",
    "    # Prepare recording CUDA memory snapshot\n",
    "    # torch.cuda.memory._record_memory_history(\n",
    "    #     max_entries=100000\n",
    "    # )\n",
    "    model.train()\n",
    "    train_epoch_loss = 0 \n",
    "    lr_rate_per_step = []\n",
    "    loss_per_step = []\n",
    "    \n",
    "\n",
    "    for step, (img, food) in tqdm(enumerate(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img = img.to(device)\n",
    "        food = food.to(device)\n",
    "        out = model(img)\n",
    "\n",
    "        loss = loss_func(out, food)\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # track lr rate per steps\n",
    "        lr_rate_per_step.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # track loss per steps \n",
    "        loss_per_step.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "    # after training is done, then print out the lr_rate_per_step\n",
    "    # plot_losses(lr_rate_per_step, f'LR rate in EPOCH #{epoch_num}')\n",
    "    # plot_losses(loss_per_step, f'Loss per step in EPOCH #{epoch_num}')\n",
    "    \n",
    "    train_step_loss = train_epoch_loss / (step + 1) \n",
    "    return train_epoch_loss, train_step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f6cd28-46d1-4607-b2c0-8f6f8fd09ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    test_epoch_loss = 0\n",
    "    correct_cnt = 0\n",
    "    total_cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (img, food) in tqdm(enumerate(test_dataloader)):\n",
    "            img = img.to(device)\n",
    "            food = food.to(device)\n",
    "            out = model(img)\n",
    "            \n",
    "            pred, idx_ = out.max(-1)\n",
    "\n",
    "            loss = loss_func(out, food)\n",
    "            \n",
    "            correct_cnt += torch.eq(food, idx_).sum().item()\n",
    "            total_cnt += food.size(0)\n",
    "\n",
    "            test_epoch_loss += loss.item()\n",
    "\n",
    "    test_step_loss = test_epoch_loss / (step + 1)\n",
    "    accuracy = correct_cnt / total_cnt * 100\n",
    "\n",
    "    return test_epoch_loss, test_step_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3910b3ff-4e9e-459c-b55d-b4011614fb8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "29it [03:31,  3.92it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m min_test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100_000_000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 6\u001b[0m     train_epoch_loss, train_step_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     test_epoch_loss, test_step_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[1;32m      9\u001b[0m     train_loss_per_epoch\u001b[38;5;241m.\u001b[39mappend(train_step_loss)\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(out, food)\n\u001b[1;32m     21\u001b[0m clip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), GRADIENT_CLIP)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# track lr rate per steps\u001b[39;00m\n\u001b[1;32m     26\u001b[0m lr_rate_per_step\u001b[38;5;241m.\u001b[39mappend(optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/_tensor.py:572\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m        used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/overrides.py:1717\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1717\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1719\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Actual training is done here\n",
    "\n",
    "min_test_loss = 100_000_000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_epoch_loss, train_step_loss = train_epoch(epoch)\n",
    "    test_epoch_loss, test_step_loss, test_accuracy = evaluate()\n",
    "\n",
    "    train_loss_per_epoch.append(train_step_loss)\n",
    "    test_loss_per_epoch.append(test_step_loss)\n",
    "    test_accuracy_per_epoch.append(test_accuracy)\n",
    "\n",
    "    logger.info(f'Epoch #{epoch} End | Train Loss: {train_step_loss} | Test Loss: {test_step_loss} | Test Accuracy: {test_accuracy:.2f}%')\n",
    "    scheduler.step()\n",
    "    # save the model parameter if it reaches the minimum test loss\n",
    "    if min_test_loss > test_step_loss:\n",
    "        min_test_loss = test_step_loss\n",
    "        model_path = model_dir / f'model_{timestamp}_{epoch}'\n",
    "        logger.info(f'Reached new min test loss. Saving the model at {model_path}')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "logger.info(f'Training Completely Ended!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2b6ad-4b01-45b2-9121-2ac09a872d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_loss_per_epoch, 'Train Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b800365-239c-4190-8006-713bd12294f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(test_loss_per_epoch, 'Test Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6bbd5-60de-4518-970b-f529e57ec6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(test_accuracy_per_epoch, 'Test Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
