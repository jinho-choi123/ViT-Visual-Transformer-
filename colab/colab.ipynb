{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051042db-5d6b-44ca-bcf2-e958012ac908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "import torch \n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path \n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fcfd41-6918-4dc7-9ab2-5a8995c1509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to google colab\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbfb624a-34e4-40b9-8d3f-1095b541f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS as device\n"
     ]
    }
   ],
   "source": [
    "# Configure device: CUDA, MPS, CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "\n",
    "    \n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17be138b-6caf-4b00-89c7-75da7946eab0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_dir: /Users/ball/Documents/workspace\n",
      "rawdata_dir: /Users/ball/Documents/workspace/rawdata\n",
      "data_dir: /Users/ball/Documents/workspace/data\n",
      "model_dir: /Users/ball/Documents/workspace/models\n",
      "log_dir: /Users/ball/Documents/workspace/logs\n"
     ]
    }
   ],
   "source": [
    "# Configure Directory\n",
    "project_dir = Path(\"/content/gdrive/MyDrive/Colab Notebooks/Food-Classifier\")\n",
    "rawdata_dir = Path(\"/content/rawdata\")\n",
    "data_dir = project_dir / \"data\"\n",
    "model_dir = project_dir / \"models\"\n",
    "log_dir = project_dir / \"logs\"\n",
    "\n",
    "rawdata_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'project_dir: {project_dir}')\n",
    "print(f'rawdata_dir: {rawdata_dir}')\n",
    "print(f'data_dir: {data_dir}')\n",
    "print(f'model_dir: {model_dir}')\n",
    "print(f'log_dir: {log_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ba89234-c8a2-4599-a095-3fcc57d97dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Logger \n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_file = log_dir / f'log_{timestamp}.log'\n",
    "\n",
    "logger = logging.getLogger('transformer_log')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03aed3-0e3b-4994-a489-6d7939a31632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input image size \n",
    "# We are going to resize the original image\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "PATCH_SIZE = 4\n",
    "IN_CHANNELS = 3 \n",
    "N_HEAD = 8\n",
    "D_MODEL = 400\n",
    "FFN_HIDDEN = 256 \n",
    "MLP_HIDDEN = 512 \n",
    "N_LAYERS = 6\n",
    "CLASS_NUM = 101\n",
    "DROP_PROB = 0.1 \n",
    "INIT_LR = 3e-2\n",
    "NUM_WORKERS=2\n",
    "WEIGHT_DECAY=0.1\n",
    "\n",
    "logger.info(f'CONFIGURATION START')\n",
    "logger.info(f'EPOCHS: {EPOCHS}')\n",
    "logger.info(f'BATCH_SIZE: {BATCH_SIZE}')\n",
    "logger.info(f'IMG_SIZE: {IMG_SIZE}')\n",
    "logger.info(f'PATCH_SIZE: {PATCH_SIZE}')\n",
    "logger.info(f'IN_CHANNELS: {IN_CHANNELS}')\n",
    "logger.info(f'N_HEAD: {N_HEAD}')\n",
    "logger.info(f'D_MODEL: {D_MODEL}')\n",
    "logger.info(f'FFN_HIDDEN: {FFN_HIDDEN}')\n",
    "logger.info(f'MLP_HIDDEN: {MLP_HIDDEN}')\n",
    "logger.info(f'N_LAYERS: {N_LAYERS}') \n",
    "logger.info(f'CLASS_NUM: {CLASS_NUM}') \n",
    "logger.info(f'DROP_PROB: {DROP_PROB}') \n",
    "logger.info(f'INIT_LR: {INIT_LR}') \n",
    "logger.info(f'NUM_WORKERS: {NUM_WORKERS}')\n",
    "logger.info(f'WEIGHT_DECAY: {WEIGHT_DECAY}')\n",
    "logger.info(f'CONFIGURATION END')\n",
    "\n",
    "# torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36aabb81-415b-4712-b535-1691832e2ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%run config.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee5d10a-8351-43bb-8fb6-bbc642eec917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c1c2c9-06fb-43e0-8c0d-1f04a1800b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Image transform function \n",
    "img_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8bf6f2-f5f9-454b-8a6e-dca3d11da4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Food101 dataset \n",
    "train_data = datasets.Food101(root=rawdata_dir, split=\"train\", download=True, transform=img_transform)\n",
    "test_data = datasets.Food101(root=rawdata_dir, split=\"test\", download=True, transform=img_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95516dc-df01-4521-a406-a25e179452e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True, generator=torch.Generator(device=device), num_workers=NUM_WORKERS)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, pin_memory=True, generator=torch.Generator(device=device), num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc7943-2be2-4715-8623-3de784f83ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "244cf584-78db-4b52-83fb-74c0dc99c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Transformer Implementation\n",
    "from torch import nn\n",
    "import torch \n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcf2b0ca-6084-4cda-8450-a74a1811203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Patch Embeddings \n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    split image into grids, and convert to a latent space vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, patch_size, in_channels, d_model, device):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size \n",
    "        self.patch_size = patch_size \n",
    "        self.in_channels = in_channels\n",
    "        self.d_model = d_model\n",
    "        # image_size should be dividable by patch_size \n",
    "        # default: image_size=400, patch_size=40\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2 \n",
    "\n",
    "        # using convolution to create non-overlapping patches\n",
    "        self.emb = nn.Conv2d(self.in_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size, device=device)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # input shape: (batch_size, in_channels, image_size, image_size) \n",
    "        # output shape: (batch_size, num_patches, out_channels)\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        # intermediate x's shape: (batch_size, seq_len, sqrt(num_patches), sqrt(num_patches))\n",
    "        \n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x \n",
    "\n",
    "## Positional Embeddings \n",
    "class PositionalEmbedding(nn.Module): \n",
    "    def __init__(self, d_model, image_size, patch_size, device):\n",
    "        super().__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.max_len = num_patches + 1\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, self.max_len, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, seq_len, _ = x.shape \n",
    "        return self.encoding[:seq_len, :]\n",
    "    \n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, d_model, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.patch_emb = PatchEmbedding(image_size, patch_size, in_channels, d_model, device)\n",
    "        self.pos_emb = PositionalEmbedding(d_model, image_size, patch_size, device)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        # Similar to BERT model, we should add CLS token to the starting of the sequence \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "    def forward(self, x): \n",
    "        batch_size, _, _, _ = x.shape\n",
    "        \n",
    "        x = self.patch_emb(x)\n",
    "        \n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # cls_token shape: (batch_size, 1, d_model)\n",
    "\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "\n",
    "        return self.dropout(x + pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e7dfb62-3160-4f85-b9d4-508ea3f06d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attention Block \n",
    "class SelfAttentionBlock(nn.Module): \n",
    "    \"\"\"\n",
    "    A attention block with scale dot product attention for \n",
    "    Query, Key, Value \n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, eps=1e-12): \n",
    "        # Since this is not a NLP, we don't need any padding-mask or look-ahead mask \n",
    "        # also, q, k, v have the same shape \n",
    "        batch_size, n_head, seq_len, d_tensor = k.shape\n",
    "\n",
    "        k_T = k.transpose(2, 3)\n",
    "        att_weight = (q @ k_T) / math.sqrt(d_tensor)\n",
    "\n",
    "        # att_weight shape: batch_size, n_head, seq_len, d_tensor \n",
    "        att_score = self.softmax(att_weight)\n",
    "\n",
    "        return att_score @ v\n",
    "\n",
    "## Multihead Attention Block \n",
    "class MultiheadAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_head, d_model): \n",
    "        super().__init__()\n",
    "        self.n_head = n_head \n",
    "        self.d_model = d_model \n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention = SelfAttentionBlock()\n",
    "\n",
    "        self.Wconcat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split(self, tensor): \n",
    "        \"\"\"\n",
    "        split tensor into n_heads \n",
    "        (batch_size, seq_len, d_model) -> (batch_size, n_head, seq_len, d_tensor)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = tensor.shape \n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "\n",
    "        tensor = tensor.reshape(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "        \n",
    "        return tensor \n",
    "\n",
    "    def concat(self, tensor): \n",
    "        \"\"\"\n",
    "        reverse of split\n",
    "        (batch_size, n_head, seq_len, d_tensor) -> (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, n_head, seq_len, d_tensor = tensor.shape \n",
    "\n",
    "        tensor = tensor.transpose(1, 2).reshape(batch_size, seq_len, n_head * d_tensor)\n",
    "        \n",
    "        return tensor \n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        # apply Wq, Wk, Wv to get q, k, v \n",
    "        query = self.split(self.Wq(x))\n",
    "        key = self.split(self.Wk(x))\n",
    "        value = self.split(self.Wv(x))\n",
    "\n",
    "        # apply attention \n",
    "        out = self.attention(query, key, value)\n",
    "\n",
    "        out = self.concat(out)\n",
    "\n",
    "        out = self.Wconcat(out)\n",
    "\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9991b777-8bfe-4c0c-8d52-fbcc5bc7e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define FeedForward Network \n",
    "class FeedForwardBlock(nn.Module): \n",
    "    def __init__(self, d_model, ffn_hidden, drop_prob=0.1): \n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, ffn_hidden)\n",
    "        self.linear2 = nn.Linear(ffn_hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4293e42-fc6f-4623-b7aa-38ff43743db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer EncoderBlock and Encoder \n",
    "class EncoderBlock(nn.Module): \n",
    "    def __init__(self, n_head, d_model, ffn_hidden, drop_prob=0.1): \n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.multihead_attn = MultiheadAttentionBlock(n_head, d_model)\n",
    "        self.ffn = FeedForwardBlock(d_model, ffn_hidden, drop_prob)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x \n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = self.multihead_attn(x)\n",
    "\n",
    "        x = self.dropout1(x + residual)\n",
    "        \n",
    "        residual = x \n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        x = self.dropout2(x + residual)\n",
    "\n",
    "        return x\n",
    "        \n",
    "class Encoder(nn.Module): \n",
    "    def __init__(self, image_size, patch_size, in_channels, n_head, d_model, ffn_hidden, n_layers, device,  drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(image_size, patch_size, in_channels, d_model, drop_prob, device)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(n_head, d_model, ffn_hidden, drop_prob) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.emb(x)\n",
    "\n",
    "        for layer in self.layers: \n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a0c2f55-38bc-4e65-a3ec-9ce992f86b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define MLP head for final classification \n",
    "class MLPHead(nn.Module): \n",
    "    def __init__(self, d_model, class_num, mlp_hidden, drop_prob): \n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, mlp_hidden)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(mlp_hidden, class_num)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.linear1(x) \n",
    "        x = self.gelu(x) \n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "152e3a41-59e1-4bb6-a3d3-07bafc35be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module): \n",
    "    \"\"\"\n",
    "    ViT Model \n",
    "    \"\"\"\n",
    "    def __init__(self, image_size, patch_size, in_channels, n_head, d_model, ffn_hidden, mlp_hidden, n_layers, class_num, device, drop_prob=0.1): \n",
    "        super().__init__()\n",
    "        seq_len = (image_size // patch_size) ** 2 + 1\n",
    "        self.encoder = Encoder(image_size, patch_size, in_channels, n_head, d_model, ffn_hidden, n_layers, device, drop_prob)\n",
    "        self.mlp_head = MLPHead(d_model, class_num, mlp_hidden, drop_prob)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.encoder(img)\n",
    "\n",
    "        logits = self.mlp_head(x[:, 0, :])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949daf37-384b-4fa2-bdf2-ba55092eb8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5710d848-ca35-4c38-b76d-3d984b7a2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Vit using Food101 dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f86868b-1470-42be-ae97-9cd3d7aa4ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%run config.ipynb\n",
    "%run data-install.ipynb\n",
    "%run ViT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97823546-22b7-45d8-845c-1a22a4c7a2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam \n",
    "from datetime import datetime \n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20415737-0e48-457c-9988-b6d42d9727ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "ViT                                                [32, 101]                 --\n",
      "├─Encoder: 1-1                                     [32, 197, 400]            --\n",
      "│    └─TransformerEmbedding: 2-1                   [32, 197, 400]            400\n",
      "│    │    └─PatchEmbedding: 3-1                    [32, 196, 400]            307,600\n",
      "│    │    └─PositionalEmbedding: 3-2               [1, 197, 400]             78,800\n",
      "│    │    └─Dropout: 3-3                           [32, 197, 400]            --\n",
      "│    └─ModuleList: 2-2                             --                        --\n",
      "│    │    └─EncoderBlock: 3-4                      [32, 197, 400]            847,856\n",
      "│    │    └─EncoderBlock: 3-5                      [32, 197, 400]            847,856\n",
      "│    │    └─EncoderBlock: 3-6                      [32, 197, 400]            847,856\n",
      "│    │    └─EncoderBlock: 3-7                      [32, 197, 400]            847,856\n",
      "│    │    └─EncoderBlock: 3-8                      [32, 197, 400]            847,856\n",
      "│    │    └─EncoderBlock: 3-9                      [32, 197, 400]            847,856\n",
      "│    └─LayerNorm: 2-3                              [32, 197, 400]            800\n",
      "├─MLPHead: 1-2                                     [32, 101]                 --\n",
      "│    └─Linear: 2-4                                 [32, 512]                 205,312\n",
      "│    └─GELU: 2-5                                   [32, 512]                 --\n",
      "│    └─Linear: 2-6                                 [32, 101]                 51,813\n",
      "│    └─Dropout: 2-7                                [32, 101]                 --\n",
      "====================================================================================================\n",
      "Total params: 5,731,861\n",
      "Trainable params: 5,731,861\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 2.10\n",
      "====================================================================================================\n",
      "Input size (MB): 19.27\n",
      "Forward/backward pass size (MB): 965.75\n",
      "Params size (MB): 22.93\n",
      "Estimated Total Size (MB): 1007.95\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = ViT(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=IN_CHANNELS,\n",
    "    n_head=N_HEAD, \n",
    "    d_model=D_MODEL, \n",
    "    ffn_hidden=FFN_HIDDEN, \n",
    "    mlp_hidden=MLP_HIDDEN, \n",
    "    n_layers=N_LAYERS, \n",
    "    class_num=CLASS_NUM, \n",
    "    device=device, \n",
    "    drop_prob=DROP_PROB,\n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "def count_parameters(model): \n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "\n",
    "logger.info(summary(model, input_size=(BATCH_SIZE, IN_CHANNELS, IMG_SIZE, IMG_SIZE)))\n",
    "print(summary(model, input_size=(BATCH_SIZE, IN_CHANNELS, IMG_SIZE, IMG_SIZE)))\n",
    "\n",
    "logger.info(f'model parameter #: {count_parameters(model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99b490b3-e88f-4599-b33b-7c1e530a517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer \n",
    "optimizer = Adam(params = model.parameters(), lr=INIT_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Setup loss function for training \n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eac9adba-1e11-4070-a4b8-6192b8bfdcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_per_epoch = []\n",
    "test_loss_per_epoch = []\n",
    "test_accuracy_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab542c29-a836-47e1-812d-ea27008b282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch_num): \n",
    "    # Prepare recording CUDA memory snapshot\n",
    "    # torch.cuda.memory._record_memory_history(\n",
    "    #     max_entries=100000\n",
    "    # )\n",
    "    model.train()\n",
    "    train_epoch_loss = 0 \n",
    "\n",
    "    for step, (img, food) in tqdm(enumerate(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img = img.to(device)\n",
    "        food = food.to(device)\n",
    "        out = model(img)\n",
    "\n",
    "        loss = loss_func(out, food)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "        if step % 200 == 0: \n",
    "            logger.info(f'EPOCH #{epoch_num} STEP #{step} | loss: {loss.item()}, avg_loss: {train_epoch_loss / (step + 1)}')\n",
    "\n",
    "    train_step_loss = train_epoch_loss / (step + 1) \n",
    "    return train_epoch_loss, train_step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04f6cd28-46d1-4607-b2c0-8f6f8fd09ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    test_epoch_loss = 0\n",
    "    correct_cnt = 0\n",
    "    total_cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (img, food) in tqdm(enumerate(test_dataloader)):\n",
    "            img = img.to(device)\n",
    "            food = food.to(device)\n",
    "            out = model(img)\n",
    "            \n",
    "            pred, idx_ = out.max(-1)\n",
    "\n",
    "            loss = loss_func(out, food)\n",
    "            \n",
    "            correct_cnt += torch.eq(food, idx_).sum().item()\n",
    "            total_cnt += food.size(0)\n",
    "\n",
    "            test_epoch_loss += loss.item()\n",
    "\n",
    "    test_step_loss = test_epoch_loss / (step + 1)\n",
    "    accuracy = correct_cnt / total_cnt * 100\n",
    "\n",
    "    return test_epoch_loss, test_step_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3910b3ff-4e9e-459c-b55d-b4011614fb8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m min_test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100_000_000\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 6\u001b[0m     train_epoch_loss, train_step_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     test_epoch_loss, test_step_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[1;32m      9\u001b[0m     train_loss_per_epoch\u001b[38;5;241m.\u001b[39mappend(train_step_loss)\n",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m     12\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m food \u001b[38;5;241m=\u001b[39m food\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(out, food)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/folders/qc/krghrss90j3_h3x_kbbgxj740000gn/T/ipykernel_75955/4094337653.py:12\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_head(x[:, \u001b[38;5;241m0\u001b[39m, :])\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/folders/qc/krghrss90j3_h3x_kbbgxj740000gn/T/ipykernel_75955/2266861831.py:37\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[0;32m---> 37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: \n\u001b[1;32m     40\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/folders/qc/krghrss90j3_h3x_kbbgxj740000gn/T/ipykernel_75955/1024444038.py:59\u001b[0m, in \u001b[0;36mTransformerEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[1;32m     57\u001b[0m     batch_size, _, _, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 59\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     cls_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# cls_token shape: (batch_size, 1, d_model)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/var/folders/qc/krghrss90j3_h3x_kbbgxj740000gn/T/ipykernel_75955/1024444038.py:24\u001b[0m, in \u001b[0;36mPatchEmbedding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# input shape: (batch_size, in_channels, image_size, image_size) \u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# output shape: (batch_size, num_patches, out_channels)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# intermediate x's shape: (batch_size, seq_len, sqrt(num_patches), sqrt(num_patches))\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/workspace/visual-transformer/.venv/lib/python3.11/site-packages/torch/utils/_device.py:106\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "# Actual training is done here\n",
    "\n",
    "min_test_loss = 100_000_000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_epoch_loss, train_step_loss = train_epoch(epoch)\n",
    "    test_epoch_loss, test_step_loss, test_accuracy = evaluate()\n",
    "\n",
    "    train_loss_per_epoch.append(train_step_loss)\n",
    "    test_loss_per_epoch.append(test_step_loss)\n",
    "    test_accuracy_per_epoch.append(test_accuracy)\n",
    "\n",
    "    logger.info(f'Epoch #{epoch} End | Train Loss: {train_step_loss} | Test Loss: {test_step_loss} | Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    # save the model parameter if it reaches the minimum test loss\n",
    "    if min_test_loss > test_step_loss:\n",
    "        min_test_loss = test_step_loss\n",
    "        model_path = model_dir / f'model_{timestamp}_{epoch}'\n",
    "        logger.info(f'Reached new min test loss. Saving the model at {model_path}')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "logger.info(f'Training Completely Ended!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e551b7f-8f6c-49dc-a3f2-8b6e1f25d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph of train_epoch_loss, test_epoch_loss, test_accuracy\n",
    "def plot_losses(loss_values, total_epochs, label):\n",
    "    x0 = list(range(1, total_epochs+1))\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot(x0, loss_values)\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a2b6ad-4b01-45b2-9121-2ac09a872d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(train_loss_per_epoch, EPOCHS, 'Train Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b800365-239c-4190-8006-713bd12294f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(test_loss_per_epoch, EPOCHS, 'Test Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6bbd5-60de-4518-970b-f529e57ec6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(test_accuracy_per_epoch, EPOCHS, 'Test Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814d310-2b56-4fe8-b3e0-e359e55e7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0233b224-b7cc-4de6-bc78-7a6a95f30724",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "%run config.ipynb\n",
    "%run ViT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd466eb-4cc6-414f-97d0-db7d8c00a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = data_dir / \"image_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1034870c-325c-40ba-b618-312f70b8c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784844d6-5137-45d6-8991-d77e53bb4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=IN_CHANNELS,\n",
    "    n_head=N_HEAD, \n",
    "    d_model=D_MODEL, \n",
    "    ffn_hidden=FFN_HIDDEN, \n",
    "    mlp_hidden=MLP_HIDDEN, \n",
    "    n_layers=N_LAYERS, \n",
    "    class_num=CLASS_NUM, \n",
    "    device=device, \n",
    "    drop_prob=DROP_PROB,\n",
    ")\n",
    "\n",
    "# load model\n",
    "model_path = model_dir / \"model_20241128_094333_1\"\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True, map_location=device))\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed0d74-f34b-4c0b-b569-da3ea256731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare single image tensor for input \n",
    "img_inference_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "img = img_inference_transform(img)\n",
    "\n",
    "# set batch_size as 1 using unsqueeze\n",
    "img.unsqueeze(0)\n",
    "print(f'shape of img: {img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262ec241-7f9f-489e-b076-96d6fc8a397d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'apple_pie\\n', 1: 'baby_back_ribs\\n', 2: 'baklava\\n', 3: 'beef_carpaccio\\n', 4: 'beef_tartare\\n', 5: 'beet_salad\\n', 6: 'beignets\\n', 7: 'bibimbap\\n', 8: 'bread_pudding\\n', 9: 'breakfast_burrito\\n', 10: 'bruschetta\\n', 11: 'caesar_salad\\n', 12: 'cannoli\\n', 13: 'caprese_salad\\n', 14: 'carrot_cake\\n', 15: 'ceviche\\n', 16: 'cheesecake\\n', 17: 'cheese_plate\\n', 18: 'chicken_curry\\n', 19: 'chicken_quesadilla\\n', 20: 'chicken_wings\\n', 21: 'chocolate_cake\\n', 22: 'chocolate_mousse\\n', 23: 'churros\\n', 24: 'clam_chowder\\n', 25: 'club_sandwich\\n', 26: 'crab_cakes\\n', 27: 'creme_brulee\\n', 28: 'croque_madame\\n', 29: 'cup_cakes\\n', 30: 'deviled_eggs\\n', 31: 'donuts\\n', 32: 'dumplings\\n', 33: 'edamame\\n', 34: 'eggs_benedict\\n', 35: 'escargots\\n', 36: 'falafel\\n', 37: 'filet_mignon\\n', 38: 'fish_and_chips\\n', 39: 'foie_gras\\n', 40: 'french_fries\\n', 41: 'french_onion_soup\\n', 42: 'french_toast\\n', 43: 'fried_calamari\\n', 44: 'fried_rice\\n', 45: 'frozen_yogurt\\n', 46: 'garlic_bread\\n', 47: 'gnocchi\\n', 48: 'greek_salad\\n', 49: 'grilled_cheese_sandwich\\n', 50: 'grilled_salmon\\n', 51: 'guacamole\\n', 52: 'gyoza\\n', 53: 'hamburger\\n', 54: 'hot_and_sour_soup\\n', 55: 'hot_dog\\n', 56: 'huevos_rancheros\\n', 57: 'hummus\\n', 58: 'ice_cream\\n', 59: 'lasagna\\n', 60: 'lobster_bisque\\n', 61: 'lobster_roll_sandwich\\n', 62: 'macaroni_and_cheese\\n', 63: 'macarons\\n', 64: 'miso_soup\\n', 65: 'mussels\\n', 66: 'nachos\\n', 67: 'omelette\\n', 68: 'onion_rings\\n', 69: 'oysters\\n', 70: 'pad_thai\\n', 71: 'paella\\n', 72: 'pancakes\\n', 73: 'panna_cotta\\n', 74: 'peking_duck\\n', 75: 'pho\\n', 76: 'pizza\\n', 77: 'pork_chop\\n', 78: 'poutine\\n', 79: 'prime_rib\\n', 80: 'pulled_pork_sandwich\\n', 81: 'ramen\\n', 82: 'ravioli\\n', 83: 'red_velvet_cake\\n', 84: 'risotto\\n', 85: 'samosa\\n', 86: 'sashimi\\n', 87: 'scallops\\n', 88: 'seaweed_salad\\n', 89: 'shrimp_and_grits\\n', 90: 'spaghetti_bolognese\\n', 91: 'spaghetti_carbonara\\n', 92: 'spring_rolls\\n', 93: 'steak\\n', 94: 'strawberry_shortcake\\n', 95: 'sushi\\n', 96: 'tacos\\n', 97: 'takoyaki\\n', 98: 'tiramisu\\n', 99: 'tuna_tartare\\n', 100: 'waffles\\n'}\n"
     ]
    }
   ],
   "source": [
    "# read the food101 classes.txt\n",
    "class_file_path = rawdata_dir / \"food-101\" / \"meta\" / \"classes.txt\"\n",
    "class_file = open(class_file_path, 'r')\n",
    "class_list = [line for line in class_file]\n",
    "class_dict = dict()\n",
    "for idx, foodname in enumerate(class_list):\n",
    "    class_dict[idx] = foodname\n",
    "\n",
    "print(f'{class_dict}')\n",
    "\n",
    "# define a helper function \n",
    "def tensor2foodid(tensor): \n",
    "    # convert tensor to foodid \n",
    "    # by picking index that has max value \n",
    "    # use argmax \n",
    "    food_id = torch.argmax(tensor, dim=-1)\n",
    "    return food_id\n",
    "\n",
    "def foodid2foodname(foodid):\n",
    "    if foodid < 0 or foodid > max(class_dict.keys()):\n",
    "        raise Exception(\"Invalid food Id for foodid2foodname function\")\n",
    "    return class_dict[foodid]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfdab9-42c7-4f97-a8cd-d8cc2bf24b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, img):\n",
    "    with torch.no_grad():\n",
    "        img = img.to(device)\n",
    "\n",
    "        out = model(img)\n",
    "        foodid = tensor2foodid(out)\n",
    "        foodname = foodid2foodname(foodid)\n",
    "\n",
    "        print(f'{foodname}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114ff41-d566-4200-9017-3f7b74fd084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
