{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051042db-5d6b-44ca-bcf2-e958012ac908",
   "metadata": {
    "id": "051042db-5d6b-44ca-bcf2-e958012ac908"
   },
   "outputs": [],
   "source": [
    "# Configurations\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75fcfd41-6918-4dc7-9ab2-5a8995c1509b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75fcfd41-6918-4dc7-9ab2-5a8995c1509b",
    "outputId": "62da6c4c-fc7a-403d-ad64-fbf75611b2ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# connect to google colab\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbfb624a-34e4-40b9-8d3f-1095b541f69e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbfb624a-34e4-40b9-8d3f-1095b541f69e",
    "outputId": "208a6a78-ec0d-4cd8-dedb-9d7c7f9c96b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA as device\n"
     ]
    }
   ],
   "source": [
    "# Configure device: CUDA, MPS, CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA as device\")\n",
    "else:\n",
    "    # Check that MPS is available\n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                  \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "                  \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU as device\")\n",
    "    else:\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS as device\")\n",
    "\n",
    "\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17be138b-6caf-4b00-89c7-75da7946eab0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17be138b-6caf-4b00-89c7-75da7946eab0",
    "outputId": "0da2348f-48c5-4729-8942-e33ff765155d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_dir: /content/gdrive/MyDrive/Colab Notebooks/Food-Classifier\n",
      "rawdata_dir: /content/rawdata\n",
      "data_dir: /content/gdrive/MyDrive/Colab Notebooks/Food-Classifier/data\n",
      "model_dir: /content/gdrive/MyDrive/Colab Notebooks/Food-Classifier/models\n",
      "log_dir: /content/gdrive/MyDrive/Colab Notebooks/Food-Classifier/logs\n"
     ]
    }
   ],
   "source": [
    "# Configure Directory\n",
    "project_dir = Path(\"/content/gdrive/MyDrive/Colab Notebooks/Food-Classifier\")\n",
    "rawdata_dir = Path(\"/content/rawdata\")\n",
    "data_dir = project_dir / \"data\"\n",
    "model_dir = project_dir / \"models\"\n",
    "log_dir = project_dir / \"logs\"\n",
    "\n",
    "rawdata_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'project_dir: {project_dir}')\n",
    "print(f'rawdata_dir: {rawdata_dir}')\n",
    "print(f'data_dir: {data_dir}')\n",
    "print(f'model_dir: {model_dir}')\n",
    "print(f'log_dir: {log_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba89234-c8a2-4599-a095-3fcc57d97dff",
   "metadata": {
    "id": "3ba89234-c8a2-4599-a095-3fcc57d97dff"
   },
   "outputs": [],
   "source": [
    "# Configure Logger\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_file = log_dir / f'log_{timestamp}.log'\n",
    "\n",
    "logger = logging.getLogger('transformer_log')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c03aed3-0e3b-4994-a489-6d7939a31632",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c03aed3-0e3b-4994-a489-6d7939a31632",
    "outputId": "6b6b7d41-f395-4e13-902b-71e6a01bf373"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformer_log:CONFIGURATION START\n",
      "INFO:transformer_log:EPOCHS: 100\n",
      "INFO:transformer_log:BATCH_SIZE: 64\n",
      "INFO:transformer_log:IMG_SIZE: 220\n",
      "INFO:transformer_log:PATCH_SIZE: 10\n",
      "INFO:transformer_log:IN_CHANNELS: 3\n",
      "INFO:transformer_log:N_HEAD: 10\n",
      "INFO:transformer_log:D_MODEL: 420\n",
      "INFO:transformer_log:FFN_HIDDEN: 256\n",
      "INFO:transformer_log:MLP_HIDDEN: 512\n",
      "INFO:transformer_log:N_LAYERS: 6\n",
      "INFO:transformer_log:CLASS_NUM: 101\n",
      "INFO:transformer_log:DROP_PROB: 0.1\n",
      "INFO:transformer_log:INIT_LR: 0.07\n",
      "INFO:transformer_log:NUM_WORKERS: 2\n",
      "INFO:transformer_log:WEIGHT_DECAY: 0\n",
      "INFO:transformer_log:CONFIGURATION END\n"
     ]
    }
   ],
   "source": [
    "# Define input image size\n",
    "# We are going to resize the original image\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 220\n",
    "PATCH_SIZE = 10\n",
    "IN_CHANNELS = 3\n",
    "N_HEAD = 10\n",
    "D_MODEL = 420\n",
    "FFN_HIDDEN = 256\n",
    "MLP_HIDDEN = 512\n",
    "N_LAYERS = 6\n",
    "CLASS_NUM = 101\n",
    "DROP_PROB = 0.1\n",
    "INIT_LR = 7e-2\n",
    "NUM_WORKERS=2\n",
    "WEIGHT_DECAY=0\n",
    "\n",
    "logger.info(f'CONFIGURATION START')\n",
    "logger.info(f'EPOCHS: {EPOCHS}')\n",
    "logger.info(f'BATCH_SIZE: {BATCH_SIZE}')\n",
    "logger.info(f'IMG_SIZE: {IMG_SIZE}')\n",
    "logger.info(f'PATCH_SIZE: {PATCH_SIZE}')\n",
    "logger.info(f'IN_CHANNELS: {IN_CHANNELS}')\n",
    "logger.info(f'N_HEAD: {N_HEAD}')\n",
    "logger.info(f'D_MODEL: {D_MODEL}')\n",
    "logger.info(f'FFN_HIDDEN: {FFN_HIDDEN}')\n",
    "logger.info(f'MLP_HIDDEN: {MLP_HIDDEN}')\n",
    "logger.info(f'N_LAYERS: {N_LAYERS}')\n",
    "logger.info(f'CLASS_NUM: {CLASS_NUM}')\n",
    "logger.info(f'DROP_PROB: {DROP_PROB}')\n",
    "logger.info(f'INIT_LR: {INIT_LR}')\n",
    "logger.info(f'NUM_WORKERS: {NUM_WORKERS}')\n",
    "logger.info(f'WEIGHT_DECAY: {WEIGHT_DECAY}')\n",
    "logger.info(f'CONFIGURATION END')\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee5d10a-8351-43bb-8fb6-bbc642eec917",
   "metadata": {
    "id": "7ee5d10a-8351-43bb-8fb6-bbc642eec917"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c1c2c9-06fb-43e0-8c0d-1f04a1800b5f",
   "metadata": {
    "id": "b8c1c2c9-06fb-43e0-8c0d-1f04a1800b5f"
   },
   "outputs": [],
   "source": [
    "# Define Image transform function\n",
    "train_img_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_img_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c8bf6f2-f5f9-454b-8a6e-dca3d11da4dc",
   "metadata": {
    "id": "7c8bf6f2-f5f9-454b-8a6e-dca3d11da4dc"
   },
   "outputs": [],
   "source": [
    "# Prepare Food101 dataset\n",
    "train_data = datasets.Food101(root=rawdata_dir, split=\"train\", download=True, transform=train_img_transform)\n",
    "test_data = datasets.Food101(root=rawdata_dir, split=\"test\", download=True, transform=test_img_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c95516dc-df01-4521-a406-a25e179452e9",
   "metadata": {
    "id": "c95516dc-df01-4521-a406-a25e179452e9"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, pin_memory=True, generator=torch.Generator(device=device), num_workers=NUM_WORKERS)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True, pin_memory=True, generator=torch.Generator(device=device), num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "244cf584-78db-4b52-83fb-74c0dc99c812",
   "metadata": {
    "id": "244cf584-78db-4b52-83fb-74c0dc99c812"
   },
   "outputs": [],
   "source": [
    "# Visual Transformer Implementation\n",
    "from torch import nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcf2b0ca-6084-4cda-8450-a74a1811203e",
   "metadata": {
    "id": "dcf2b0ca-6084-4cda-8450-a74a1811203e"
   },
   "outputs": [],
   "source": [
    "## Patch Embeddings\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    split image into grids, and convert to a latent space vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, patch_size, in_channels, d_model, device):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "        self.d_model = d_model\n",
    "        # image_size should be dividable by patch_size\n",
    "        # default: image_size=400, patch_size=40\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "        # using convolution to create non-overlapping patches\n",
    "        self.emb = nn.Conv2d(self.in_channels, self.d_model, kernel_size=self.patch_size, stride=self.patch_size, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input shape: (batch_size, in_channels, image_size, image_size)\n",
    "        # output shape: (batch_size, num_patches, out_channels)\n",
    "\n",
    "        x = self.emb(x)\n",
    "        # intermediate x's shape: (batch_size, seq_len, sqrt(num_patches), sqrt(num_patches))\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "## Positional Embeddings\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, image_size, patch_size, device):\n",
    "        super().__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.max_len = num_patches + 1\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, self.max_len, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, seq_len, _ = x.shape\n",
    "        return self.encoding[:seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, d_model, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.patch_emb = PatchEmbedding(image_size, patch_size, in_channels, d_model, device)\n",
    "        self.pos_emb = PositionalEmbedding(d_model, image_size, patch_size, device)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        # Similar to BERT model, we should add CLS token to the starting of the sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, _, _ = x.shape\n",
    "\n",
    "        x = self.patch_emb(x)\n",
    "\n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # cls_token shape: (batch_size, 1, d_model)\n",
    "\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "\n",
    "        return self.dropout(x + pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e7dfb62-3160-4f85-b9d4-508ea3f06d69",
   "metadata": {
    "id": "9e7dfb62-3160-4f85-b9d4-508ea3f06d69"
   },
   "outputs": [],
   "source": [
    "## Attention Block\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A attention block with scale dot product attention for\n",
    "    Query, Key, Value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, eps=1e-12):\n",
    "        # Since this is not a NLP, we don't need any padding-mask or look-ahead mask\n",
    "        # also, q, k, v have the same shape\n",
    "        batch_size, n_head, seq_len, d_tensor = k.shape\n",
    "\n",
    "        k_T = k.transpose(2, 3)\n",
    "        att_weight = (q @ k_T) / math.sqrt(d_tensor)\n",
    "\n",
    "        # att_weight shape: batch_size, n_head, seq_len, d_tensor\n",
    "        att_score = self.softmax(att_weight)\n",
    "\n",
    "        return att_score @ v\n",
    "\n",
    "## Multihead Attention Block\n",
    "class MultiheadAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_head, d_model):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention = SelfAttentionBlock()\n",
    "\n",
    "        self.Wconcat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split(self, tensor):\n",
    "        \"\"\"\n",
    "        split tensor into n_heads\n",
    "        (batch_size, seq_len, d_model) -> (batch_size, n_head, seq_len, d_tensor)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = tensor.shape\n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "\n",
    "        tensor = tensor.reshape(batch_size, seq_len, self.n_head, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def concat(self, tensor):\n",
    "        \"\"\"\n",
    "        reverse of split\n",
    "        (batch_size, n_head, seq_len, d_tensor) -> (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, n_head, seq_len, d_tensor = tensor.shape\n",
    "\n",
    "        tensor = tensor.transpose(1, 2).reshape(batch_size, seq_len, n_head * d_tensor)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # apply Wq, Wk, Wv to get q, k, v\n",
    "        query = self.split(self.Wq(x))\n",
    "        key = self.split(self.Wk(x))\n",
    "        value = self.split(self.Wv(x))\n",
    "\n",
    "        # apply attention\n",
    "        out = self.attention(query, key, value)\n",
    "\n",
    "        out = self.concat(out)\n",
    "\n",
    "        out = self.Wconcat(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9991b777-8bfe-4c0c-8d52-fbcc5bc7e663",
   "metadata": {
    "id": "9991b777-8bfe-4c0c-8d52-fbcc5bc7e663"
   },
   "outputs": [],
   "source": [
    "## Define FeedForward Network\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, ffn_hidden)\n",
    "        self.linear2 = nn.Linear(ffn_hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4293e42-fc6f-4623-b7aa-38ff43743db3",
   "metadata": {
    "id": "e4293e42-fc6f-4623-b7aa-38ff43743db3"
   },
   "outputs": [],
   "source": [
    "# Define Transformer EncoderBlock and Encoder\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_head, d_model, ffn_hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.multihead_attn = MultiheadAttentionBlock(n_head, d_model)\n",
    "        self.ffn = FeedForwardBlock(d_model, ffn_hidden, drop_prob)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.multihead_attn(x)\n",
    "\n",
    "        x = self.dropout1(x + residual)\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        x = self.dropout2(x + residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, n_head, d_model, ffn_hidden, n_layers, device,  drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(image_size, patch_size, in_channels, d_model, drop_prob, device)\n",
    "        self.layers = nn.ModuleList([EncoderBlock(n_head, d_model, ffn_hidden, drop_prob) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a0c2f55-38bc-4e65-a3ec-9ce992f86b4e",
   "metadata": {
    "id": "0a0c2f55-38bc-4e65-a3ec-9ce992f86b4e"
   },
   "outputs": [],
   "source": [
    "## Define MLP head for final classification\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, d_model, class_num, mlp_hidden, drop_prob):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, mlp_hidden)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(mlp_hidden, class_num)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "152e3a41-59e1-4bb6-a3d3-07bafc35be39",
   "metadata": {
    "id": "152e3a41-59e1-4bb6-a3d3-07bafc35be39"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT Model\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size, patch_size, in_channels, n_head, d_model, ffn_hidden, mlp_hidden, n_layers, class_num, device, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        seq_len = (image_size // patch_size) ** 2 + 1\n",
    "        self.encoder = Encoder(image_size, patch_size, in_channels, n_head, d_model, ffn_hidden, n_layers, device, drop_prob)\n",
    "        self.mlp_head = MLPHead(d_model, class_num, mlp_hidden, drop_prob)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.encoder(img)\n",
    "\n",
    "        logits = self.mlp_head(x[:, 0, :])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5710d848-ca35-4c38-b76d-3d984b7a2493",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5710d848-ca35-4c38-b76d-3d984b7a2493",
    "outputId": "30c95f72-6e7b-4b8e-c8e0-a17984685063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Train the Vit using Food101 dataset\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97823546-22b7-45d8-845c-1a22a4c7a2e3",
   "metadata": {
    "id": "97823546-22b7-45d8-845c-1a22a4c7a2e3"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20415737-0e48-457c-9988-b6d42d9727ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20415737-0e48-457c-9988-b6d42d9727ed",
    "outputId": "cba70f30-3f70-4778-dbad-e0b2de26df91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformer_log:====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "ViT                                                [64, 101]                 --\n",
      "├─Encoder: 1-1                                     [64, 485, 420]            --\n",
      "│    └─TransformerEmbedding: 2-1                   [64, 485, 420]            420\n",
      "│    │    └─PatchEmbedding: 3-1                    [64, 484, 420]            126,420\n",
      "│    │    └─PositionalEmbedding: 3-2               [1, 485, 420]             203,700\n",
      "│    │    └─Dropout: 3-3                           [64, 485, 420]            --\n",
      "│    └─ModuleList: 2-2                             --                        --\n",
      "│    │    └─EncoderBlock: 3-4                      [64, 485, 420]            923,836\n",
      "│    │    └─EncoderBlock: 3-5                      [64, 485, 420]            923,836\n",
      "│    │    └─EncoderBlock: 3-6                      [64, 485, 420]            923,836\n",
      "│    │    └─EncoderBlock: 3-7                      [64, 485, 420]            923,836\n",
      "│    │    └─EncoderBlock: 3-8                      [64, 485, 420]            923,836\n",
      "│    │    └─EncoderBlock: 3-9                      [64, 485, 420]            923,836\n",
      "│    └─LayerNorm: 2-3                              [64, 485, 420]            840\n",
      "├─MLPHead: 1-2                                     [64, 101]                 --\n",
      "│    └─Linear: 2-4                                 [64, 512]                 215,552\n",
      "│    └─GELU: 2-5                                   [64, 512]                 --\n",
      "│    └─Linear: 2-6                                 [64, 101]                 51,813\n",
      "│    └─Dropout: 2-7                                [64, 101]                 --\n",
      "====================================================================================================\n",
      "Total params: 6,141,761\n",
      "Trainable params: 6,141,761\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 4.29\n",
      "====================================================================================================\n",
      "Input size (MB): 37.17\n",
      "Forward/backward pass size (MB): 4972.10\n",
      "Params size (MB): 24.57\n",
      "Estimated Total Size (MB): 5033.84\n",
      "====================================================================================================\n",
      "INFO:transformer_log:model parameter #: 6141761\n"
     ]
    }
   ],
   "source": [
    "model = ViT(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=IN_CHANNELS,\n",
    "    n_head=N_HEAD,\n",
    "    d_model=D_MODEL,\n",
    "    ffn_hidden=FFN_HIDDEN,\n",
    "    mlp_hidden=MLP_HIDDEN,\n",
    "    n_layers=N_LAYERS,\n",
    "    class_num=CLASS_NUM,\n",
    "    device=device,\n",
    "    drop_prob=DROP_PROB,\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "logger.info(summary(model, input_size=(BATCH_SIZE, IN_CHANNELS, IMG_SIZE, IMG_SIZE)))\n",
    "# print(summary(model, input_size=(BATCH_SIZE, IN_CHANNELS, IMG_SIZE, IMG_SIZE)))\n",
    "\n",
    "logger.info(f'model parameter #: {count_parameters(model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99b490b3-e88f-4599-b33b-7c1e530a517f",
   "metadata": {
    "id": "99b490b3-e88f-4599-b33b-7c1e530a517f"
   },
   "outputs": [],
   "source": [
    "# Setup optimizer\n",
    "optimizer = Adam(params = model.parameters(), lr=INIT_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define learning rate scheduler.\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "                                        last_epoch=-1)\n",
    "\n",
    "# Setup loss function for training\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eac9adba-1e11-4070-a4b8-6192b8bfdcdf",
   "metadata": {
    "id": "eac9adba-1e11-4070-a4b8-6192b8bfdcdf"
   },
   "outputs": [],
   "source": [
    "train_loss_per_epoch = []\n",
    "test_loss_per_epoch = []\n",
    "test_accuracy_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab542c29-a836-47e1-812d-ea27008b282a",
   "metadata": {
    "id": "ab542c29-a836-47e1-812d-ea27008b282a"
   },
   "outputs": [],
   "source": [
    "def train_epoch(epoch_num):\n",
    "    # Prepare recording CUDA memory snapshot\n",
    "    # torch.cuda.memory._record_memory_history(\n",
    "    #     max_entries=100000\n",
    "    # )\n",
    "    model.train()\n",
    "    train_epoch_loss = 0\n",
    "\n",
    "    for step, (img, food) in tqdm(enumerate(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img = img.to(device)\n",
    "        food = food.to(device)\n",
    "        out = model(img)\n",
    "\n",
    "        loss = loss_func(out, food)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # print(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        train_epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            logger.info(f'EPOCH #{epoch_num} STEP #{step} | avg_loss: {train_epoch_loss / (step + 1)}')\n",
    "\n",
    "    train_step_loss = train_epoch_loss / (step + 1)\n",
    "    return train_epoch_loss, train_step_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04f6cd28-46d1-4607-b2c0-8f6f8fd09ee4",
   "metadata": {
    "id": "04f6cd28-46d1-4607-b2c0-8f6f8fd09ee4"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    test_epoch_loss = 0\n",
    "    correct_cnt = 0\n",
    "    total_cnt = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, (img, food) in tqdm(enumerate(test_dataloader)):\n",
    "            img = img.to(device)\n",
    "            food = food.to(device)\n",
    "            out = model(img)\n",
    "\n",
    "            pred, idx_ = out.max(-1)\n",
    "\n",
    "            loss = loss_func(out, food)\n",
    "\n",
    "            correct_cnt += torch.eq(food, idx_).sum().item()\n",
    "            total_cnt += food.size(0)\n",
    "\n",
    "            test_epoch_loss += loss.item()\n",
    "\n",
    "    test_step_loss = test_epoch_loss / (step + 1)\n",
    "    accuracy = correct_cnt * 100 / total_cnt\n",
    "\n",
    "    return test_epoch_loss, test_step_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3910b3ff-4e9e-459c-b55d-b4011614fb8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 758
    },
    "id": "3910b3ff-4e9e-459c-b55d-b4011614fb8d",
    "outputId": "9867c120-3e31-447b-d0ff-4ae9dbca0e39",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]INFO:transformer_log:EPOCH #0 STEP #0 | avg_loss: 4.655364990234375\n",
      "100it [01:23,  1.26it/s]INFO:transformer_log:EPOCH #0 STEP #100 | avg_loss: 12.95995273212395\n",
      "200it [02:44,  1.23it/s]INFO:transformer_log:EPOCH #0 STEP #200 | avg_loss: 8.819634148137487\n",
      "300it [04:04,  1.25it/s]INFO:transformer_log:EPOCH #0 STEP #300 | avg_loss: 7.4298293645991835\n",
      "400it [05:25,  1.23it/s]INFO:transformer_log:EPOCH #0 STEP #400 | avg_loss: 6.7332649623366665\n",
      "500it [06:46,  1.24it/s]INFO:transformer_log:EPOCH #0 STEP #500 | avg_loss: 6.314924201089703\n",
      "600it [08:07,  1.24it/s]INFO:transformer_log:EPOCH #0 STEP #600 | avg_loss: 6.035846799859985\n",
      "700it [09:28,  1.23it/s]INFO:transformer_log:EPOCH #0 STEP #700 | avg_loss: 5.836774547157886\n",
      "800it [10:48,  1.24it/s]INFO:transformer_log:EPOCH #0 STEP #800 | avg_loss: 5.68674116753758\n",
      "900it [12:09,  1.24it/s]INFO:transformer_log:EPOCH #0 STEP #900 | avg_loss: 5.5701863413777915\n",
      "1000it [13:30,  1.24it/s]INFO:transformer_log:EPOCH #0 STEP #1000 | avg_loss: 5.477202515502076\n",
      "1100it [14:51,  1.24it/s]INFO:transformer_log:EPOCH #0 STEP #1100 | avg_loss: 5.400895381168708\n",
      "1183it [15:59,  1.23it/s]\n",
      "394it [02:37,  2.50it/s]\n",
      "INFO:transformer_log:Epoch #0 End | Train Loss: 5.347918432240554 | Test Loss: 4.638351887010681 | Test Accuracy: 0.99%\n",
      "INFO:transformer_log:Reached new min test loss. Saving the model at /content/gdrive/MyDrive/Colab Notebooks/Food-Classifier/models/model_20241130_182445_0\n",
      "0it [00:00, ?it/s]INFO:transformer_log:EPOCH #1 STEP #0 | avg_loss: 4.608131408691406\n",
      "100it [01:21,  1.24it/s]INFO:transformer_log:EPOCH #1 STEP #100 | avg_loss: 4.636606891556542\n",
      "200it [02:42,  1.24it/s]INFO:transformer_log:EPOCH #1 STEP #200 | avg_loss: 4.636537027596241\n",
      "300it [04:02,  1.24it/s]INFO:transformer_log:EPOCH #1 STEP #300 | avg_loss: 4.637034324316487\n",
      "400it [05:23,  1.24it/s]INFO:transformer_log:EPOCH #1 STEP #400 | avg_loss: 4.637044742517638\n",
      "441it [05:57,  1.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-9a18b0d36e51>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtest_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_step_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-8ca37f7cce9c>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# print(optimizer.param_groups[0]['lr'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_addcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_exp_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             bias_correction1 = [\n\u001b[0m\u001b[1;32m    593\u001b[0m                 \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m             ]\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             bias_correction1 = [\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_state_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             ]\n\u001b[1;32m    595\u001b[0m             bias_correction2 = [\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_device_constructors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# NB: This is directly called from C++ in torch/csrc/Device.cpp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Actual training is done here\n",
    "\n",
    "min_test_loss = 100_000_000\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_epoch_loss, train_step_loss = train_epoch(epoch)\n",
    "    test_epoch_loss, test_step_loss, test_accuracy = evaluate()\n",
    "\n",
    "    train_loss_per_epoch.append(train_step_loss)\n",
    "    test_loss_per_epoch.append(test_step_loss)\n",
    "    test_accuracy_per_epoch.append(test_accuracy)\n",
    "    scheduler.step()\n",
    "\n",
    "    logger.info(f'Epoch #{epoch} End | Train Loss: {train_step_loss} | Test Loss: {test_step_loss} | Test Accuracy: {test_accuracy:.2f}%')\n",
    "    # save the model parameter if it reaches the minimum test loss\n",
    "    if min_test_loss > test_step_loss:\n",
    "        min_test_loss = test_step_loss\n",
    "        model_path = model_dir / f'model_{timestamp}_{epoch}'\n",
    "        logger.info(f'Reached new min test loss. Saving the model at {model_path}')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "logger.info(f'Training Completely Ended!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e551b7f-8f6c-49dc-a3f2-8b6e1f25d78b",
   "metadata": {
    "id": "8e551b7f-8f6c-49dc-a3f2-8b6e1f25d78b"
   },
   "outputs": [],
   "source": [
    "# Plot the graph of train_epoch_loss, test_epoch_loss, test_accuracy\n",
    "def plot_losses(loss_values, total_epochs, label):\n",
    "    x0 = list(range(1, total_epochs+1))\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot(x0, loss_values)\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2b6ad-4b01-45b2-9121-2ac09a872d89",
   "metadata": {
    "id": "b0a2b6ad-4b01-45b2-9121-2ac09a872d89"
   },
   "outputs": [],
   "source": [
    "plot_losses(train_loss_per_epoch, EPOCHS, 'Train Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b800365-239c-4190-8006-713bd12294f5",
   "metadata": {
    "id": "4b800365-239c-4190-8006-713bd12294f5"
   },
   "outputs": [],
   "source": [
    "plot_losses(test_loss_per_epoch, EPOCHS, 'Test Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6bbd5-60de-4518-970b-f529e57ec6f3",
   "metadata": {
    "id": "27e6bbd5-60de-4518-970b-f529e57ec6f3"
   },
   "outputs": [],
   "source": [
    "plot_losses(test_accuracy_per_epoch, EPOCHS, 'Test Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a814d310-2b56-4fe8-b3e0-e359e55e7b07",
   "metadata": {
    "id": "a814d310-2b56-4fe8-b3e0-e359e55e7b07"
   },
   "outputs": [],
   "source": [
    "# Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd466eb-4cc6-414f-97d0-db7d8c00a395",
   "metadata": {
    "id": "bfd466eb-4cc6-414f-97d0-db7d8c00a395"
   },
   "outputs": [],
   "source": [
    "img_path = data_dir / \"image_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1034870c-325c-40ba-b618-312f70b8c4a8",
   "metadata": {
    "id": "1034870c-325c-40ba-b618-312f70b8c4a8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784844d6-5137-45d6-8991-d77e53bb4d2e",
   "metadata": {
    "id": "784844d6-5137-45d6-8991-d77e53bb4d2e"
   },
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=IN_CHANNELS,\n",
    "    n_head=N_HEAD,\n",
    "    d_model=D_MODEL,\n",
    "    ffn_hidden=FFN_HIDDEN,\n",
    "    mlp_hidden=MLP_HIDDEN,\n",
    "    n_layers=N_LAYERS,\n",
    "    class_num=CLASS_NUM,\n",
    "    device=device,\n",
    "    drop_prob=DROP_PROB,\n",
    ")\n",
    "\n",
    "# load model\n",
    "model_path = model_dir / \"model_20241128_094333_1\"\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True, map_location=device))\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed0d74-f34b-4c0b-b569-da3ea256731c",
   "metadata": {
    "id": "5eed0d74-f34b-4c0b-b569-da3ea256731c"
   },
   "outputs": [],
   "source": [
    "# prepare single image tensor for input\n",
    "img_inference_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "img = img_inference_transform(img)\n",
    "\n",
    "# set batch_size as 1 using unsqueeze\n",
    "img.unsqueeze(0)\n",
    "print(f'shape of img: {img.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262ec241-7f9f-489e-b076-96d6fc8a397d",
   "metadata": {
    "id": "262ec241-7f9f-489e-b076-96d6fc8a397d"
   },
   "outputs": [],
   "source": [
    "# read the food101 classes.txt\n",
    "class_file_path = rawdata_dir / \"food-101\" / \"meta\" / \"classes.txt\"\n",
    "class_file = open(class_file_path, 'r')\n",
    "class_list = [line for line in class_file]\n",
    "class_dict = dict()\n",
    "for idx, foodname in enumerate(class_list):\n",
    "    class_dict[idx] = foodname\n",
    "\n",
    "print(f'{class_dict}')\n",
    "\n",
    "# define a helper function\n",
    "def tensor2foodid(tensor):\n",
    "    # convert tensor to foodid\n",
    "    # by picking index that has max value\n",
    "    # use argmax\n",
    "    food_id = torch.argmax(tensor, dim=-1)\n",
    "    return food_id\n",
    "\n",
    "def foodid2foodname(foodid):\n",
    "    if foodid < 0 or foodid > max(class_dict.keys()):\n",
    "        raise Exception(\"Invalid food Id for foodid2foodname function\")\n",
    "    return class_dict[foodid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfdab9-42c7-4f97-a8cd-d8cc2bf24b4e",
   "metadata": {
    "id": "89cfdab9-42c7-4f97-a8cd-d8cc2bf24b4e"
   },
   "outputs": [],
   "source": [
    "def inference(model, img):\n",
    "    with torch.no_grad():\n",
    "        img = img.to(device)\n",
    "\n",
    "        out = model(img)\n",
    "        foodid = tensor2foodid(out)\n",
    "        foodname = foodid2foodname(foodid)\n",
    "\n",
    "        print(f'{foodname}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114ff41-d566-4200-9017-3f7b74fd084c",
   "metadata": {
    "id": "e114ff41-d566-4200-9017-3f7b74fd084c"
   },
   "outputs": [],
   "source": [
    "inference(model, img)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
